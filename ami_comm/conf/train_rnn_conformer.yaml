# network architecture
# encoder related
# etype: blstmp     # encoder architecture type
elayers: 12
eunits: 2048
eprojs: 512 # same as conformer output
# subsample: "3_1_1_1_1" # skip every n frame from input to nth layers
# decoder related
dlayers: 2
dunits: 1024
context-residual: true
# attention related
atype: location
adim: 1024
awin: 5
aheads: 4
aconv-chans: 10
aconv-filts: 100

# conformer related
adim-conformer: 512
aheads-conformer: 4
model-module: espnet.nets.pytorch_backend.e2e_asr_cfmlas:E2E
transformer-attn-dropout-rate: 0.0
transformer-input-layer: conv2d
transformer-warmup-steps: 25000 # ???
transformer-encoder-activation-type: swish
transformer-encoder-pos-enc-layer-type: rel_pos
transformer-encoder-selfattn-layer-type: rel_selfattn
transformer-length-normalized-loss: false
transformer-lr: 2.5
macaron-style: true
use-cnn-module: true
cnn-module-kernel: 31

# hybrid CTC/attention
mtlalpha: 0.0

# label smoothing
lsm-type: unigram
lsm-weight: 0.1

# minibatch related
batch-size: 24
maxlen-in: 500  # if input length  > maxlen_in, batchsize is automatically reduced
maxlen-out: 150 # if output length > maxlen_out, batchsize is automatically reduced

# optimization related
sortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs
epochs: 120
patience: 0
sortagrad: 1
accum-grad: 6
grad-clip: 5
opt: noam

# scheduled sampling option
sampling-probability: 0.0

# Extra
dropout-rate: 0.1
dropout-rate-decoder: 0.1
weight-decay: 0.0
# report-interval-iters: 1
