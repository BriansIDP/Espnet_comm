# network architecture
backend: pytorch
model-module: espnet.nets.pytorch_backend.e2e_asr_cfmlas:E2E
# encoder related
elayers: 16
eunits: 2048
eprojs: 512 # this must equal to transformer-adim
transformer-adim: 512
transformer-aheads: 4
transformer-attn-dropout-rate: 0.0
transformer-input-layer: conv2d
transformer-encoder-activation-type: swish
transformer-encoder-pos-enc-layer-type: rel_pos
transformer-encoder-selfattn-layer-type: rel_selfattn
macaron-style: true
use-cnn-module: true
cnn-module-kernel: 31
# decoder related
dlayers: 2
dunits: 1024
# attention related
adim: 1024
atype: location
aconv-chans: 10
aconv-filts: 100

# hybrid CTC/attention
mtlalpha: 0.0

# label smoothing
lsm-type: unigram
lsm-weight: 0.1
dropout-rate: 0.1
dropout-rate-decoder: 0.1
weight-decay: 0.0
# ema-decay: 0.999
context-residual: true

# minibatch related
batch-size: 20  
maxlen-in: 512
maxlen-out: 150
# maxioratio: 300
# minioratio: 6

# optimization related
sortagrad: 1
accum-grad: 6
grad-clip: 5
opt: noam
epochs: 120
patience: 0
weight-noise-std: 0.00
weight-noise-start: 30000
transformer-lr: 2.5
transformer-warmup-steps: 25000

# scheduled sampling option
sampling-probability: 0.0
# report-interval-iters: 1

# KB related
meetingpath: /home/dawna/gs534/espnet-debug/egs/ami/ami_lextree/data/KBs/AMItrainClean_unigram200suffix
meetingKB: true
dictfile: /home/dawna/gs534/espnet-debug/egs/ami/ami_lextree/data/KBs/bpe_dict_unigram200suffix.txt
lm-odim: 512
KBlextree: true
PtrGen: true
PtrSche: 30
smoothprob: 0.6
# init-full-model: /home/dawna/gs534/espnet-debug/egs/ami/ami_lextree/exp/init_models/baseline.ep.20
acousticonly: true
attn_dim: 256
# dynamicKBs: 0
